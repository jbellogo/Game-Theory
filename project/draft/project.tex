\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{algorithm} %
\usepackage{algpseudocode}
\usepackage{amsfonts}

\usepackage{tcolorbox}
\newtcolorbox{greenbox}[1]{colback=green!5!white,colframe=green!75!black,fonttitle=\bfseries,title=#1}


\usepackage{amsthm}
\usepackage{bm}

\newtheorem{definition}{(Definition)}
\newtheorem{theorem}{(Theorem)}
\newtheorem{lemma}{(Lemma)}
\newtheorem{corollary}{(Corollary)}

\newtheorem*{remark}{Remark}

\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{argmin} % no space, limits underneath in displays



\setlength{\parindent}{0pt} % for getting rid of first line indentation


% ================================================
% =====================  BEGIN DOCUMENT 
% ================================================

\title{Game Theory Project}
\author{Juan Pablo Bello Gonzalez}
\date{September 2024}

\begin{document}


\maketitle

\section{introduction}

extensive form games to model imperfect information games. They involve sequential decisions when players choose actions at differnt times (Prof Bryce Extensive Games).
To represent these interactions we can use the extensive form gametree which lets us encode the timing information of a sequential game

"the key idea is tht if player are making decisions at different times, then each point where a player makes a decision is represented as a node in a tree. We'll label the node with the player who is making the decision at that point. Also each level on the tree alternates players. And decisions avaliable to a player when they are making a decision will result in branches steming from that node. Those branches may lead to other decision nodes for various players in the game or they may lead to a terminal outcome represented by a leaf in the tree where as usual we will have a payoff for each of the players, in the case of poker and Khun poker this payoff will be zero sum. "

these levels of the tree correspond to a player making a decision and then player 2 reacting to that decision. This is a computationally intensive method (do poker tree start) even after symmetries removed. 

How strategies and equillibria relate to this model?
- We think of a strategy in an extensive form game as a complete contingent plan of actin that a player would do at any of their decision nodes, given the history wich is visible to them. 
So a strategy for player i takes as input any of the decision nodes for that player and produces an action that the player would play at that possition. 


Counter factual regret minimization. 

How this can be used to model poker and weakly solve poker 


\subsubsection{modeling imperfect information}

"To model imperfect information, the histories of each player $d$ are partitioned into a collection $d$
of so-called information sets. Each information set $d$
 groups together histories that Player i cannot
distinguish between when he or she acts  (MIT LECTUREs)" For instance, in the case of Kuhn poker, An information set of player 1 is ${JQ, JK}$ which occurs after the draw and before any betting, at this point in time, Player 1 only has knowelde of what eh drew, J in this case. But his information set is bigger since, from his poov, P1 cannot know if P2 has Q or K. 



\section{Defining the game}
According to (REGRET MINIMIZATION..)
A formal definition of an extensive game includes the following:
\begin{itemize}
\item A finite set of two players $N={P_1, P_2}$
\item A finite set of ordered action histories $H$.
In the case of Khun poker, there are a two distinct action sets, those corresponding to chance draw $A_1 = {KJ, KQ, QJ, QK, JQ, JK}$ with the ordering encoding the player ie $KJ  $
 
 In the case of Khun poker H = set of ordered subsets of $C \times A \times A$ where $C = { (a,b) : a \in Cards \and b \in Cards \and a\neq b}$ 
Some examples $(Q,J,b,b)$
$Z \subset H$ is the set of terminal histories. for instance $(QJ,b,b)$, $(QJ,p)$, etc. 
$A(h)$ are actions avaliable after a non-terminal history $h \in H$.

\item A function $P: H\Z \rightarro N$ that determines the player whose turn it is to act at the end of a given history. 

EXPLANATION: 
In the Khun poker definition, 
$P([(c_1,c_2)]) = P1$ encodes that after the draw from chance, it is player 1's turn to act. 
$P([(c_1,c_2), b]) = P2$ encodes that after the draw from change and player 1's betting action, it is the turn of player 2. 

\item A function $f_c$. which assignes a probability distribution over at every decision point, ie at node corresponding to player i's decision, given the history the probabiity that they go transition to the avaliable options. This is just the definition of that distribution, we would like to find the optimal such distribution which maximizes utility?

For each player $i \in N$, a partition 
\end{itemize}

\subsubsection{A strategy in an extensive form game}
'Loosely speaking, a strategy describes how to act in every single possible situation. It is a recipe for how to act in entire game, a lookup table you can read your actions from. (INT8)'

For player i, a strategy should provide a probability sdistribution over actions avaliable to him at every information set where he is expected to perform an action, so not including terminal nodes. For khun poker: 


\subsubsection{Nash Equillibrium}
By Nash's Theorem, any finite game is guaratneed to have a, possibly mixed, NE. This includes poker and Khun poker. 

\subsubsection{Information sets}
For definining imperfect information sets, players do not have knoweldge of the entire state, player's perspectives are called information sets. 
"Information set is a set of game states (game tree nodes) that are not distinguishable for a player."

\subsubsection{No Regret Learning}
regret is central to \textbf{online learning}, which deals with repeatedly making decisions in imperfect information environemtns. 

We must index by time, this means we get the right answers and can evaluate expert predictions.

Assume we have expert predictions. We wish to distribute trust. After the environemnt reacts, it will reveal the real outcome (sequential/online structure) and so we obtain a loss vector that evaluates out N experts.

"Regret at time t is a difference between our algorithm total loss and the best single expert loss" 

\subsubsection{Regret matching as an example of no-regret learning algorithm}
EXPERTS are actions (ie you shuold fold, call, bet)
WE can compute cummulatve regret wrt an expert i at time t, expressing how we regret not listening to particular epert i. and we update that experts weight.

\subsubsection{connecting no-regret learning and game theory}
This regret matching algorothm can be used to model something that evolves with time, but the issue with games is that we would like to input the game state/history at some point into the regret,

Since regret goes from costs to utility, we now wish to maximize it?

Both players in fact will adapt to one another via no-regret learning. So how do we keep track of the regret for each plater?

\textbf{THEOREM:}
If two no-regret algorithms play a zero-sum game against one another for T
 iterations with average regret less than ϵ
, then their average strategies approximate Nash Equilibrium (up to 2ϵ) 

\subsubsection{Average Overall Regret}



\end{document}
